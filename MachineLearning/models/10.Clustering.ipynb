{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0054cc19-59b9-494c-93b8-c57f2833d255",
   "metadata": {},
   "source": [
    "# K-means\n",
    "---\n",
    "## 1 Formulation mathématique\n",
    "\n",
    "### 1.1 Enonciation du problème \n",
    "\n",
    "La méthode des $K$ moyennes (centres mobiles) est une méthode d'apprentissage automatique non supervisé qui consiste à regrouper les échantillons d'un *dataset* en fonction de leurs caractéristiques. $K$ est un nombre réel fixé à l'avance (i.e. un hyperparamètre qu'il faut parcimonieusement bien choisir). C'est un algorithme itératif qui se déroule de la manière suivante : \n",
    "\n",
    "1. A l'initialisation, on tire $K \\in X$ clusters (classes) de manière aléatoire qui forment les $K$ centres initiaux $\\lambda_1, \\dots, \\lambda_k$. Ces centres correspondent à des points dans l'espace.\n",
    "2. On construit une partition en affectant à chaque points de $X$ le centroïde le plus proche\n",
    "\n",
    "$$\n",
    "P_k = \\{ x_i \\quad | \\quad || x_i - \\lambda_i || =< || x_i - \\lambda_p||² , \\forall p \\in \\{ 1, \\dots, K \\}\\}\n",
    "$$\n",
    "\n",
    "3. Les nouveaux centroïdes sont mis à jours (centre de gravité de la nouvelle partition)\n",
    "\n",
    "$$\n",
    "\\lambda_k = \\frac{1}{|P_k|} \\sum_{x \\in P_k} x_i\n",
    "$$\n",
    "\n",
    "4. On répète l'algorithme jusqu'à la convergence (i.e. lorsque les centroïdes ne bougent plus)\n",
    "\n",
    "### 1.2 Exemple sur le dataset Iris\n",
    "\n",
    "L'animation ci-dessous permet de mettre en exergue le fonctionnement de l'algorithme $K$-means sur le *dataset* Iris. On applique premièrement une standardisation sur les quatres features du jeu de données (sepal et petal lenght / wight) puis une PCA qui regroupe que la composante n1 comme étant le score de dimension des sépales et la compopsante n2 comme étant le score de dimension des pétales. Les échantillons (vecteurs dans $\\mathcal{R}⁴$) sont donc projetés dans $\\mathcal{R}^2$. On représente chaque espèces par des couleurs différentes. A l'initialisation, les centroïdes sont désignés aléatoirement et sont actualisés en recalculant les centres de gravité de chaque classe. On s'arrête lorsque les $\\lambda_k, \\quad k \\in \\{ 1, K \\}$ convergent.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/kmeans_iris.gif\" alt=\"a\" width=\"550\" height=\"500\">\n",
    "</div>\n",
    "\n",
    "### 1.3 Comment trouver $K$ empiriquement ?\n",
    "\n",
    "Si nous n'avons pas d'apriori sur la valeur de l'hyperparamètre $K$, on entraîne un $K$-means en faisant varier $K$ sur un intervalle pour en évaluer les métriques. La valeur du $K$ est trouvé graphiquement à l'aide de la méthode du coude.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/methodecoude.png\" alt=\"a\" width=\"350\" height=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9379c-f32a-45ae-8a5b-de897108e896",
   "metadata": {},
   "source": [
    "## 2 Des exemples en Python\n",
    "---\n",
    "#### 2.1 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b8d88-10d2-4dda-b904-6b517e58c306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4e38f5-1008-409b-bf0c-b3161d1b852f",
   "metadata": {},
   "source": [
    "# Classification Ascendante Hierarchique (CAH)\n",
    "---\n",
    "## 1 Formulation mathématique\n",
    "\n",
    "### 1.1 Classification ascendante\n",
    "\n",
    "1. A l'initialisation, on considère chaque points comme un cluster.\n",
    "\n",
    "2. On regroupe les deux points les plus proches entre eux en calculant la matrice des distances\n",
    "\n",
    "$$\n",
    "D = (d_{i,j})_{i, j \\in \\times \\mathbb{N} \\times \\mathbb{N}}, \\in \\mathcal{M}_{n}(\\mathbb{R})  \\quad d_{i,j} = \\mathcal{D}(x_i, x_j)\n",
    "$$\n",
    "\n",
    "$\\mathcal{D}$ est une distance. On prend en général la distance euclidienne, i.e. $\\mathcal{D}(x_i, x_j) = || x_i - x_j ||_2^2 = \\sum_{k=1}^{n}(x_{i,k} - x_{j,k})^2$\n",
    "\n",
    "3. A chaque étapes, on fusionne les clusters\n",
    "\n",
    "4. On repète l'algorithme jusqu'à n'avoir plus qu'un cluster\n",
    "\n",
    "### 1.2 Classification descendante\n",
    "\n",
    "1. On part d'un seul cluster\n",
    "\n",
    "2. On divise le cluster en deux en avec un critère de séparation (variance des points intra-cluster, distance entre points)\n",
    "\n",
    "3. A chaque étapes, on divise les clusters en deux avec un critère de séparation\n",
    "\n",
    "4. On répète l'algorithme jusqu'à un critère d'arrêt (jusqu'à avoir autant de clusters que de points ou en fonction de la variance intra-classes) \n",
    "\n",
    "\n",
    "Rmq : On peut aussi tracer un dendrogramme descendant.\n",
    "\n",
    "### 1.3 Exemple 1 : construction du dendrogramme.\n",
    "\n",
    "Voici un exemple expliquant la constuction du dendrogramme. On part du *dataset* suivant :\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/exempleCAHdataset.png\" alt=\"a\" width=\"150\" height=\"150\">\n",
    "</div>\n",
    "\n",
    "à chaque étapes de l'algorithme, on calcule la matrice des distances\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/exempleCAHalgo.png\" alt=\"a\" width=\"800\" height=\"500\">\n",
    "</div>\n",
    "\n",
    "On obtient un dendrogramme qui résume le partitionnement.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/exempleCAHdendrogramme.png\" alt=\"a\" width=\"150\" height=\"150\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### 1.4 Exemple 2 sur le dataset Iris.\n",
    "\n",
    "L'animation ci-dessous permet de mettre en exergue le fonctionnement de l'algorithme CAH en ascendant dans le plan qui synthètise le *dataset* Iris (cf PCA). Le graphique de droite est un *dendrogramme*, qui permet de hierarchiser les regroupements statistiques d'instances entre elles. Par exemple, si l'on possède un *dataset* de $n$ *features*, les variables les plus corrélées entre elles sont regroupées en premier. Ensuite, chaque groupes de variables corrélées sont regroupées entre elles et ainsi de suite. Ici, on fait de même avec la distance entre les points. \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/cah_iris_dendrogramme.gif\" alt=\"a\" width=\"1000\" height=\"650\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f778a6-12a8-4eac-a3dd-6483ca357d2e",
   "metadata": {},
   "source": [
    "## 2 Des exemples en Python\n",
    "---\n",
    "#### 2.1 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b50e1-905e-4c38-ae56-e0297e565925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b6cbc03-004d-4208-a4ab-3201ef431424",
   "metadata": {},
   "source": [
    "# Quantification vectorielle\n",
    "---\n",
    "\n",
    "### 1.1 Introduction\n",
    "\n",
    "La quantification vectorielle est une méthode de compression avec pertes basé sur la compression par blocs (vecteurs de données). Pour un signal ou une image $s$, on partitionne $s$ en $K$ blocs $b_k, k \\in \\{1;K\\}, K \\in \\mathbb{N}^*$ et on remplace chaque valeurs de $b_k$ par son centroïde $c_k$. L'ensemble des centroïdes $\\mathcal{C}$ de chaque blocs $b_k$ est appelé **codebook**. D'un point de vue algorithmie, on a les étapes suivantes :\n",
    "\n",
    "1. On partitionne $s$ de telle sorte à avoir $K$ blocs $b_k$ (parties de $s$)\n",
    "\n",
    "$$\n",
    "s = (b_1, \\dots, b_K)\n",
    "$$\n",
    "\n",
    "2. On vectorise les blocs (ou partitions $b_k$ de $s$)\n",
    "\n",
    "$$\n",
    "\\quad \\forall k \\in \\{1:K\\}, \\quad b_k \\leftarrow \\text{vect}(b_k)\n",
    "$$\n",
    "\n",
    "3. On remplace chaque points $x_i \\in b_k$ par leurs centroïdes $c_k$ :\n",
    "\n",
    "$$\n",
    "\\forall x_i \\in b_k, \\forall k \\in \\{1:K\\}, \\quad x_i \\leftarrow c_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{C} = (c_1, \\dots, c_K) = \\{ (c_k)_{k \\in \\{1;K\\}} \\quad | \\quad c_k = \\frac{1}{|b_k|} \\sum_{x_i \\in b_k} x_i \\}\n",
    "$$\n",
    "\n",
    "### 1.2 Un problème d'optimisation\n",
    "\n",
    "Pour chaque valeurs $x$ en entrée, on cherche le centroïde le plus proche pour lui assigner sa valeur. On cherche alors à résoudre le problème d'optimisation suivant : \n",
    "\n",
    "$$\n",
    "\\min_{c_k \\in \\mathcal{C}} d(x, c_k)\n",
    "$$\n",
    "\n",
    "ou $d(\\bullet, \\bullet)$ est une distance (en général euclidienne)\n",
    "\n",
    "### 1.3 Exemple sur une image \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/Kmeans/vector_quantization.gif\" alt=\"a\" width=\"250\" height=\"350\">\n",
    "</div>\n",
    "\n",
    "### 1.4 Cellule de Voronoï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b0d249-773a-461d-b40e-20f84d4f7d0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
