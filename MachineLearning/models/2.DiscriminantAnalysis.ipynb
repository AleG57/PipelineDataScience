{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57f1aab-65ad-417c-a2b3-0183086888aa",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/DiscriminantAnalysis/qda_iris.png\" alt=\"a\" width=\"750\" height=\"500\">\n",
    "</div>\n",
    "\n",
    "## 1.1 Formulation mathématique\n",
    "\n",
    "\n",
    "\n",
    "La QDA est une méthode statistique de classification supervisée. Pour chaque classes $k$ associée à une valeur de sortie $y \\in \\mathcal{Y} = \\{ k |  i \\in \\mathbb{N}^*\\}$, on considère que ces classes suivent une répartition gaussienne et que chaque classe a sa propre matrice de covariance. i.e.\n",
    "\n",
    "$$\n",
    "\\boxed{X | Y = k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "Avec : \n",
    "\n",
    "$$\n",
    "\\quad \\mu_k \\in \\mathbb{R}^n, \\Sigma_k \\in \\mathcal{M}_{n}(\\mathbb{R}), \\forall k, n \\in \\mathbb{N}^* \\times \\mathbb{N}^*\n",
    "$$\n",
    "\n",
    "* Un estimateur de la moyenne $\\mu_k$ est donné par $\\hat{\\mu_k} = \\frac{1}{n_k}\\sum_{i=1}^{n_k}x_i$\n",
    "* De même pour la matrice de covariance, $\\Sigma_k$, $\\hat{\\Sigma_k} = \\frac{1}{n_k - 1}\\sum_{i=1}^{n}(x_i - \\hat{\\mu_k})(x_i - \\hat{\\mu_k})^T$\n",
    "\n",
    "La probabilité *a priori* d'appartenir à une classe vaut donc :\n",
    "\n",
    "$$\n",
    "\\hat{\\pi_k} = \\frac{n_k}{n}\n",
    "$$\n",
    "\n",
    "QDA cherche alors à retourner le score le plus élevé évalué pour chaque classe, ce qui correspond à retourner la classe la plus probable en fonction des données d'entrées pour un échantillon de validation. Autrement dit, un estimateur de la classe à prédire $y$, noté $\\hat{y} \\in \\mathcal{Y}$ est : \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{arg} \\max_{k \\in \\mathbb{N}^*} \\delta_k(x)\n",
    "$$\n",
    "\n",
    "Avec : \n",
    "\n",
    "$$\n",
    "\\delta_k(x) =\n",
    "\\;\n",
    "\\underbrace{\n",
    "\\log(\\pi_k)\n",
    "}_{\\text{Pénalisation / biais a priori}}\n",
    "\\underbrace{\n",
    "-\\frac{1}{2}(x-\\mu_k)^T \\Sigma_k^{-1}(x-\\mu_k)\n",
    "}_{\\text{Distance de Mahalanobis}}\n",
    "\\;\n",
    "-\\frac{1}{2}\\log|\\Sigma_k|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Retour sur la matrice de covariance et de corrélation\n",
    "\n",
    "* **Matrice de corrélation**. Pour un ensemble de variables $(X_n)_{n \\in \\mathbb{N}}$ Une matrice de corrélation est une matrice symétrique $\\Sigma$ tel que :\n",
    "\n",
    "$$\n",
    "\\Sigma = \n",
    "\\begin{bmatrix}\n",
    "r_{1,1} & \\dots & r_{1,n} \\\\\n",
    "\\vdots & r_{i,j} & \\vdots \\\\\n",
    "r_{n,1} & \\dots & r_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "avec les $(r_{i,j})_{i, j \\in \\mathbb{N} \\times \\mathbb{N}}$ les coefficients de corrélation tels que :\n",
    "\n",
    "$$\n",
    "r_{i,j} = \\text{Corr}(X_i,X_j) = \\frac{\\text{Cov}(X_i, X_j)}{\\sigma_{X_{i}} \\sigma_{X_{j}}} \\quad \\forall i, j \\in \\mathbb{N}^* \\times \\mathbb{N}^*\n",
    "$$\n",
    "\n",
    "avec $\\text{Cov}(X_i, X_j) = \\mathbb{E}[(X_i - \\mathbb{E}(X_i))(X_j - \\mathbb{E}(X_j))]$. Pour des échantillons $\\{(x_i^n,x_j^n) | n \\in \\mathbb{N}\\}$, un estimateur du coefficient de corrélation de $r_{i,j}$ noté $\\hat{r_{i,j}}$ est donné par les relations suivantes\n",
    "\n",
    "$$\n",
    "\\hat{r_{i,j}} = \\frac{\\hat{\\sigma_{X_i,X_j}}}{\\hat{\\sigma_{X_{i}}} \\hat{\\sigma_{X_{j}}}}\n",
    "$$\n",
    "\n",
    "Avec : \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_{X,Y} = \\frac{1}{N}\\sum_{k=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y}), \\quad \\hat{\\sigma}_{S} = \\sqrt{\\frac{1}{N}\\sum_{k=1}^{N}(s_i - \\bar{s})²}, \\quad \\bar{s} = \\frac{1}{N}\\sum_{k=1}^{N}s_i, \\quad S = X \\quad \\text{ou} \\quad S = Y\n",
    "$$\n",
    "\n",
    "* **Matrice de covariance**. Pour un ensemble de variables $(X_n)_{n \\in \\mathbb{N}}$ Une matrice de covariance est une matrice symétrique $\\Sigma$ tel que :\n",
    "\n",
    "$$\n",
    "\\Sigma = \n",
    "\\begin{bmatrix}\n",
    "\\sigma_{1,1} & \\dots & \\sigma_{1,n} \\\\\n",
    "\\vdots & \\sigma_{i,j} & \\vdots \\\\\n",
    "\\sigma_{n,1} & \\dots & \\sigma_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "avec les $(\\sigma_{i,j})_{i, j \\in \\mathbb{N} \\times \\mathbb{N}}$ la covariance entre les variables $X_i$ et $X_j$ tels que :\n",
    "\n",
    "$$\n",
    "\\sigma_{i,j} = \\text{Cov}(X_i, X_j) = \\mathbb{E}[(X_i - \\mathbb{E}(X_i))(X_j - \\mathbb{E}(X_j))], \\quad \\forall i, j \\in \\mathbb{N}^* \\times \\mathbb{N}^*\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Illustration avec le *dataset* Iris de QDA\n",
    "\n",
    "Voici une animation qui permet d'illustrer le fonctionnement de QDA sur 2 *features* du *dataset* Iris (4 *features* initialement). QDA sépare non linéairement le *dataset* en fonction des classes. \n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/DiscriminantAnalysis/qda_iris.gif\" alt=\"a\" width=\"600\" height=\"450\">\n",
    "</div>\n",
    "\n",
    "## 2. Proposition d'implémentation de Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46408fda-daf8-4bfa-8479-ee047d8a198c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0551547-2e0e-439a-b64a-5f24ba77d1c4",
   "metadata": {},
   "source": [
    "## 3. Des exemples en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bab6f-bade-4d09-a390-ccd041e36cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81668b17-93f8-410b-b953-865d34c0c619",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)\n",
    "---\n",
    "## 1.1 Formulation mathématique\n",
    "\n",
    "La LDA suit le même paradigme que QDA mais en considérant la matrice de covariance commune à toutes les classes, i.e. pour $K$ classes :\n",
    "\n",
    "$$\n",
    "\\boxed{X | Y = k \\sim \\mathcal{N}(\\mu_k, \\Sigma)}\n",
    "$$\n",
    "\n",
    "Avec : \n",
    "\n",
    "$$\n",
    "\\quad \\mu_k \\in \\mathbb{R}^n, \\Sigma \\in \\mathcal{M}_{n}(\\mathbb{R}), \\forall n \\in \\mathbb{N}^*\n",
    "$$\n",
    "\n",
    "* Un estimateur de la moyenne $\\mu_k$ est donné par $\\hat{\\mu_k} = \\frac{1}{n_k}\\sum_{i=1}^{n_k}x_i$\n",
    "* De même pour la matrice de covariance, $\\Sigma$, $\\hat{\\Sigma} = \\frac{1}{n - K}\\sum_{k=1}^{K}(x_i - \\hat{\\mu_k})(x_i - \\hat{\\mu_k})^T$\n",
    "\n",
    "La fonction discriminante est donc : \n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{arg} \\max_{k \\in \\mathbb{N}^*} \\delta_k(x)\n",
    "$$\n",
    "\n",
    "Avec : \n",
    "\n",
    "$$\n",
    "\\delta_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log(\\pi_k)\n",
    "$$\n",
    "\n",
    "Cette méthode permet de séparer le plus possible la moyenne des groupes en tenant compte de la variance globale $\\Sigma^{-1}$\n",
    "\n",
    "\n",
    "## 1.2 Illustration avec le *dataset* Iris de LDA\n",
    "\n",
    "Voici une animation qui projette le dataset Iris (4 *features*) sur le plan en utilisant LDA. LDA applique une réduction de dimension et projette l'espace de dimension 4 dans l'espace LDA (axes LDA1 et LDA2) en maximisant la séparation entre les classes. Contrairement à QDA, la séparation est ici linéaire.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"src/pics/DiscriminantAnalysis/lda_iris.gif\" alt=\"a\" width=\"600\" height=\"450\">\n",
    "</div>\n",
    "\n",
    "## 1.3 Comparaison entre QDA et LDA\n",
    "\n",
    "| Critère                            | LDA (Linear Discriminant Analysis)                                                           | QDA (Quadratic Discriminant Analysis)                                                                                 |\n",
    "| ---------------------------------- | -------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Hypothèse sur la covariance**    | Une seule matrice de covariance $\\Sigma$ commune à toutes les classes     | Une matrice de covariance $\\Sigma_k$ pour chaque classe  |\n",
    "| **Frontière de décision**          | Linéaire                                                                  | Quadratique    |\n",
    "| **Nombre de paramètres à estimer** | Faible                                                                    | Elevé (une covariance par classe) |\n",
    "| **Risque d'overfitting**           | Faible, stable sur petits échantillons                                    | Plus élevé si peu de données par classe   |\n",
    "| **Performance**                    | Bonne si les classes ont des formes similaires                            | Meilleure si les classes ont des formes très différentes  |\n",
    "| **Réduction de dimension**         | Peut être utilisé pour projeter les données sur un espace de dimension réduite  | Pas vraiment utilisé pour la réduction de dimension   |\n",
    "| **Intuition statistique**          | Cherche une projection qui maximise la séparation linéaire entre classes  | Séparation quadratique |\n",
    "| **Visualisation typique**          | Frontières droites entre classes                                          | Frontières  elliptiques |\n",
    "\n",
    "\n",
    "## 2. Proposition d'implémentation de Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f84965-e296-48f2-9eb0-4535af39c0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62b9fd0b-766c-415e-849f-055a92aed17e",
   "metadata": {},
   "source": [
    "## 3. Des exemples en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223fd81-7a5a-4247-a783-f9413eaa1b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
