\chapter{Formulation mathématique d'un \textit{dataset}}


\newpage

\chapter{Métriques}
\section{Régression}

\begin{itemize}

    \item \textbf{Mean Absolute Error} : 
        \begin{equation}
            MSE(Y) = \frac{1}{n}\sum_{i = 1}^{n}|y_{i} - \hat{y_{i}}|
        \end{equation} \\
    \item \textbf{Root Mean Squarred Error} : 
        \begin{equation}
            MSE(Y) = \frac{1}{n}\sum_{i = 1}^{n}(y_{i} - \hat{y_{i}})^{2}
        \end{equation} \\

    \item 
\end{itemize}

\section{Classification}

\begin{itemize}

    \item \textbf{Accuracy score} : 
        \begin{equation}
            \text{Accuracy} = \frac{VP + VN}{VP + VN + FN + FP} = 1 - exactitude
        \end{equation} 

    Avec : \newline

    \begin{itemize}
        \item  VP : Vrai négatif   \\
        \item  FP : Faux positifs  \\
        \item  FN : Faux négatifs  \\
        \item  VP : Vrais positifs \\
    \end{itemize} 

    \item \textbf{Matrice de confusion} \newline
    Une matrice de confusion recense le nombre de faux positifs, faux négatifs, vrai positifs et vrai négatifs 
    sous forme de tableau. Les éléments de cette matrice $2 \times 2$ veulent dire ceci : \newline

    \begin{itemize}
        \item  Vrai négatif (Réel 0, Prédit 0)   \\
        \item  Faux positifs (Réel 0, Prédit 1)  \\
        \item  Faux négatifs (Réel 1, Prédit 0)  \\ 
        \item  Vrais positifs (Réel 1, Prédit 1) \\
    \end{itemize}

    La prédiction est la valeur que le modèle prédit tandis que réel est la valeur réelle, qui peut être observée à *posteriori*
    Par exemple, une matrice de confusion peut ressembler à ça : \newline


    \begin{table}[h!]
    \scriptsize
    \begin{tabularx}{\textwidth}{|C|C|C|}
    \hline
      &\textbf{Prédit : 0}   & \textbf{Prédit : 1}  \\
    \hline
    Réel : 0 & 87 & 13 \\
    \hline
    Réel : 1 & 18 & 61 \\
    \hline
    \end{tabularx}
    \caption{Exemple de matrice de confusion}
    \end{table}

    Ici, le modèle a prédit que 87 + 18 = 105 personnes n'ont pas survécues à l'accident sur les 87 + 13 = 100 personnes qui n'ont réellement pas survécues.
    De même, le modèle prédit que 13 + 61 = 74 personnes on survécues sur 10 + 61 = 71 qui ont réelement survécues. \\

\end{itemize}

\newpage

\chapter{Synthèse des modèles et leurs usages}

\begin{table}[h!]
\scriptsize
\begin{tabularx}{\textwidth}{|C|C|C|C|C|C|C|}
\hline
\textbf{Modèle}                                    & \textbf{Quand l'utiliser}                                                                 & \textbf{Paradigme}                   & \textbf{Formule}                                                                & \textbf{Quantité à minimiser}                                                                      & \textbf{Préparation des données}  \\
\hline

Régression linéaire                                & Relation linéaire entre $X_i$ et $X_j$                                                    & Régression statistique               & $\hat{y} = X\beta + \varepsilon$                                                & $||y - X\beta||^2$                                                                                 & Standardisation \\
\hline

Régression Ridge                                   & Régression avec pénalisation $L_2$ (sélection de variables)                               & Régression statistique               & $\hat{y} = X\beta + \varepsilon$                                                & $||y-X\beta||^2 + \lambda ||\beta||^2$                                                             & Standardisation \\
\hline

Régression Lasso                                   & Régression avec pénalisation $L_1$ (sélection de variables avec annulation)               & Régression statistique               & $\hat{y} = X\beta + \varepsilon$                                                & $||y-X\beta||^2 + \lambda ||\beta||_1$                                                             & Standardisation \\
\hline

Elastic Net                                        & Régression Ridge + Lasso                                                                  & Régression statistique               & $\hat{y} = X\beta + \varepsilon$                                                & $||y-X\beta||^2 + \lambda(\alpha ||\beta||_1 +(1-\alpha)||\beta||_2^2)$                            & Standardisation \\
\hline

Régression logistique                              & Classification binaire                                                                    & Classification binaire               & $\hat{y} = \sigma(X\beta)$                                                      & $-\sum y\log(\hat y)$                                                                              & Standardisation \\
\hline

Régression logistique multinomiale                 & Classification                                                                            & Classification                       &                                                                                 &                                                                                                    & Standardisation \\
\hline

Régression polynomiale                             & Régression non linéaire                                                                   & Régression statistique               & $\hat{y} = \sum_{k=0}^{d} \beta_k x^k$                                          & $\sum_{i=1}^{n}(y_i - \hat{y_i})^2$                                                                & Standardisation \\
\hline

Régression splines                                 & Régression non linéaire, robuste, locale                                                  & Régression statistique               &                                                                                 &                                                                                                    & -               \\
\hline

Régression additive généralisée (GAM)              & Régression fortement non linéaire, robuste, locale                                        & Régression statistique               &                                                                                 &                                                                                                    & -               \\
\hline

Régression isotonique                              & Régression non linéaire monotone                                                          & Régression statistique               &                                                                                 &                                                                                                    & Standardisation \\
\hline

Régression Généralisée (GLM)                       & Régression non linéaire                                                                   & Régression statistique               &                                                                                 &                                                                                                    & Standardisation \\
\hline

Quantile regression                                & Régression fortement non linéaire, robuste, présence d'hétéroscédasticité                 & Régression statistique               &                                                                                 & pinball loss                                                                                       & Standardisation \\
\hline

Processus gaussiens (GP)                           & Régression fortement non linéaire                                                         & Régression statistique               &                                                                                 &                                                                                                    & - \\
\hline

Huber regression                                   & Régression fortement non linéaire, robuste, insensible aux outliers                       & Régression statistique               &                                                                                 & huber loss                                                                                         & - \\
\hline

Régression PLS                                     & Régression non linéaire avec analyse des composantes principales                          & Régression non supervisée            &                                                                                 &                                                                                                    & Standardisation \\
\hline

KNN regression                                     & Régression locale / ponctuelle avec beaucoup de données                                   & Régression statistique               & $\hat{y} = \frac{1}{k}\sum_{i \in \mathcal{N}_k(x)} y_i$                        & $\sum_{i=1}^{n}(y_i - \hat{y_i})^2$                                                                & Standardisation \\
\hline

\end{tabularx}
\caption{Synthèse des modèles de Machine Learning - régression}
\end{table}

\newpage

\begin{table}[h!]
\scriptsize
\begin{tabularx}{\textwidth}{|C|C|C|C|C|C|C|}
\hline
\textbf{Modèle}                                    & \textbf{Quand l'utiliser}                                                                 & \textbf{Paradigme}                   & \textbf{Formule}                                                                & \textbf{Quantité à minimiser}                                                                      & \textbf{Préparation des données}  \\
\hline

SVM (Support Vector Machine)                       & Classification fortement non linéaire à marge dure                                        & Classification supervisée            & $\hat{y_i} = \text{sign}({w^{T}x_{i} + b})$                                     & $\frac{1}{2}||w||^2$                                                                               & Standardisation \\
\hline

CSV-C (C-Suport Vector Classification)             & Régression fortement non linéaire avec tolérance aux marges (hyperparamètre non borné)    & Classification supervisée            & $\hat{y_i} = \text{sign}(\sum_{i=1}^{n} \alpha_{i}y_{i}K(x_{i},x_{j}) + b)$     & $\frac{1}{2}||w||^2 + C\sum_{i=1}^{n}\xi_{i}$                                                      & Standardisation \\
\hline

$\nu$-SVM ($\nu$-Suport Vector Machine)            & Régression fortement non linéaire avec tolérance aux marges (hyperparamètre borné)        & Classification supervisée            & $\hat{y_i} = \text{sign}( \sum_{i=1}^{n} \alpha_{i}y_{i}K(x_{i},x_{j}) + b)$    & $\frac{1}{2}||w||^2 - \nu \rho + \frac{1}{n}\sum_{i=1}^{n}\xi_{i}$                                 & Standardisation \\
\hline

SVR / $\epsilon$-SVM (Support Vector Regressor)    & Régression fortement non linéaire                                                         & Régression supervisée                & $\hat{y_i} = \sum_{i=1}^{n}(\alpha_{i} - \alpha_{i}^{*}) K(x_{i},x) + b$        & $\frac{1}{2}||w||^2 + C\sum_{i=1}^{n}(\xi_{i} + \xi_{i}^{*})$                                      & Standardisation \\
\hline

$\nu$-SVR ($\nu$-Support Vector Regressor)         & Régression fortement non linéaire                                                         & Régression supervisée                & $\hat{y_i} = \sum_{i=1}^{n}(\alpha_{i} - \alpha_{i}^{*}) K(x_{i},x) + b$        & $\frac{1}{2}||w||^2 + C(\nu\epsilon + \frac{1}{n}\sum_{i=1}^{n}(\xi_{i} + \xi_{i}^{*}))$           & Standardisation \\
\hline

\end{tabularx}
\caption{Synthèse des modèles de Machine Learning - vecteurs support}
\end{table}

\newpage

\begin{table}[h!]
\scriptsize
\begin{tabularx}{\textwidth}{|C|C|C|C|C|C|C|}
\hline
\textbf{Modèle}                                    & \textbf{Quand l'utiliser}                                                                 & \textbf{Paradigme}                   & \textbf{Formule}                                                                & \textbf{Quantité à minimiser}                                                                      & \textbf{Préparation des données}  \\
\hline

Decision tree                                      & Classification                                                                            & Classification supervisée            & Partitionnement récursif                                                        & $\sum_{i=1}^{n}(y_i - \hat{y_i})²$ dans les feuilles                                               & -               \\
\hline

Random Forest                                      & Classification, robustesse, relations non linéaires                                       & Méthode ensembliste (classification) & Moyenne ou classe majoritaire des arbres                                        & $\sum_{i=1}^{n}(y_i - \hat{y_i})²$ out-of-bag                                                      & -               \\
\hline

Gradient Boosting / XGBoost                        & Classification, robustesse, relations fortement non linéaire                              & Ensemble boosting                    & Actualisation d'un arbre faible                                                 & $-\sum y\log(\hat y)$                                                                              & -               \\
\hline

\end{tabularx}
\caption{Synthèse des modèles de Machine Learning - méthodes d'ensemble}
\end{table}

\newpage

\begin{table}[h!]
\scriptsize
\begin{tabularx}{\textwidth}{|C|C|C|C|C|C|C|}
\hline
\textbf{Modèle}      & \textbf{Quand l'utiliser}                                           & \textbf{Paradigme}                   & \textbf{Formule}                                                                & \textbf{Quantité à minimiser}         & \textbf{Préparation des données}  \\
\hline

AR                   & Séries temporelles                                                  & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline

MA                   & Séries temporelles                                                  & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline

ARMA                 & Séries temporelles                                                  & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline

ARIMA                & Séries temporelles                                                  & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline

SARIMA               & Séries temporelles, saisonnarité                                    & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline

SARIMAX              & Séries temporelles, saisonnarité avec variables exogènes            & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline


\end{tabularx}
\caption{Synthèse des modèles de Machine Learning - processus autorégressifs}
\end{table}

\newpage

\begin{table}[h!]
\scriptsize
\begin{tabularx}{\textwidth}{|C|C|C|C|C|C|C|}
\hline
\textbf{Modèle}      & \textbf{Quand l'utiliser}                                           & \textbf{Paradigme}                   & \textbf{Formule}                                                                & \textbf{Quantité à minimiser}         & \textbf{Préparation des données}  \\
\hline

AR                   & Séries temporelles                                                  & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline


\end{tabularx}
\caption{Synthèse des modèles de Machine Learning - clustering}
\end{table}

\newpage

\begin{table}[h!]
\scriptsize
\begin{tabularx}{\textwidth}{|C|C|C|C|C|C|C|}
\hline
\textbf{Modèle}      & \textbf{Quand l'utiliser}                                           & \textbf{Paradigme}                   & \textbf{Formule}                                                                & \textbf{Quantité à minimiser}         & \textbf{Préparation des données}  \\
\hline

AR                   & Séries temporelles                                                  & Modèle autorégressif                 &                                                                                 & -                                     & Stationnarité                      \\
\hline


\end{tabularx}
\caption{Synthèse des modèles de Machine Learning - réduction de dimension}
\end{table}